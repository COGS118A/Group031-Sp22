{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AemvBFtk0_R"
   },
   "source": [
    "# COGS 118A - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sk0NeWWhk0_z"
   },
   "source": [
    "# Image Classification of ASL Hand Gestures\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Rahul Ravi\n",
    "- Rachel Doron\n",
    "- Yohan Kim\n",
    "- Brian Ripley\n",
    "- Tianze Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3dK6mxwk0_2"
   },
   "source": [
    "# Abstract \n",
    "\n",
    "Our goal is to accurately classify hand signs from American Sign Language (ASL) alphabet. We used the Sign Language MNIST dataset which consists of 784 total pixels per sample which represent a single 28x28 greyscaled pixel image of ASL hand gestures. The train set includes 27,455 cases and the testing set includes 7172 cases. We implemented an SVM model and tried different combinations of hyperparameters using GridSearchCV. We also used Convolutional Neural Networks (CNN) to classify the ASL hand gestures. We manipulated the number of layers and filters in our CNN to find an optimal model. This has potential application to help faciliate better communication between people who are deaf/hard of hearing and people who do not know ASL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpcBWD_1k0_5"
   },
   "source": [
    "# Background\n",
    "\n",
    "American Sign Language is expressed with hand signs. It is the primary language in the United States for people who are hard of hearing or deaf. In the United States, there are around 10 million people who are hard of hearing and around 1 million people who are functionally deaf<sup><a href=\"https://pubmed.ncbi.nlm.nih.gov/16177267/\">[1]</a></sup>. Creating a machine learning algorithm that can recognize ASL hand signs can help these people. Some prior work that has occurred with American Sign Language recognition includes an implementation of an ASL translator on a web application based on a convolutional neural network classifier<sup><a href=\"http://cs231n.stanford.edu/reports/2016/pdfs/214_Report.pdf\">[2]</a></sup>. Due to a lack of variation in their dataset they were unable to reproduce the validation accuracies they observed during training when they were testing. They hypothesize that with a more robust dataset, their models would be able to generalize more accurately. Creating a machine learning algorithm that can accurately recognize American Sign Language signs can allow people who use ASL to communicate with people who do not know sign language. This will lead them to be able to communicate with a wider range of people that they were not able to do so previously. \n",
    "\n",
    "[1]: https://pubmed.ncbi.nlm.nih.gov/16177267/\n",
    "[2]: http://cs231n.stanford.edu/reports/2016/pdfs/214_Report.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5kqJGukSk0_8"
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "Given images of ASL hand signs, we want classify them with optimal accuracy. To achieve this we will be training our models on the 24 (excluding J and Z because they require motion) different characters of the ASL alphabet. We are going to use 27,455 of real world hand signs of the ASL alphabet in the form of images that we will generalize to a 28 by 28 pixel image. We want to then deploy models to learn on this data and predict the letter of a given hand sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OjL8pask0_-",
    "outputId": "6fa47c21-75b0-42a1-ace1-ea9d01f30628"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/american_sign_language.png\" width=\"600\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import image module\n",
    "from IPython.display import Image\n",
    "  \n",
    "# get the image\n",
    "Image(url=\"images/american_sign_language.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0M2XAeGrk1AK"
   },
   "source": [
    "# Data\n",
    "\n",
    "We are going to be using Sign Language MNIST dataset, located at https://www.kaggle.com/datasets/datamunge/sign-language-mnist. This dataset comprises of 34000 oberservations of various hand signs of the ASL that look this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxpap_cGk1AN",
    "outputId": "ae313c8f-7f48-49d2-b09a-965566b385e5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/asl_color.png\" width=\"600\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"images/asl_color.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFJ5YQWDk1AS"
   },
   "source": [
    "The data distributor has preprocessed the data, yielding in 784 gray-scaled pixels to analyze. Grayscale images may look like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0IqIRKek1AV",
    "outputId": "500dd4e6-fb25-4195-c2e3-a8468551ecdb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"images/asl_gray.png\" width=\"600\" height=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url=\"images/asl_gray.png\", width=600, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GIL1TGEk1AY"
   },
   "source": [
    "The dataset consists of 24 classes of letters (excluding J and Z which require motion). Each training and test case represents a label (0-25) as a one-to-one map for each alphabetic letter A-Z (and no cases for 9=J or 25=Z). The training data includes 27,455 images and the test data includes 7172 images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHh-vCUPk1Ab"
   },
   "source": [
    "First we checked if there are any corrupted values in our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NB52kYick1Ad"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data = pd.read_csv('sign_mnist_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PCFKUrbck1Ag",
    "outputId": "f15c7e57-0fac-42c0-a091-7f3fae4ec019"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PAcaGwsk1Ak",
    "outputId": "99faa592-1b6b-42f2-818c-033b2f400520"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('int64')], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHjMVW51k1An"
   },
   "outputs": [],
   "source": [
    "column_val = 'pixel{value}'\n",
    "for i in range(1, 785):\n",
    "    if (data[column_val.format(value=i)] < 0).any() or (data[column_val.format(value=i)] > 255).any():\n",
    "        print(\"Unexpected value encountered in pixel column {value}\".format(value=i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJHwofzIk1Aq"
   },
   "source": [
    "As we can see from these lines above, our dataset looks clean and we do not see any unexpected values from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSSFOpzmk1As",
    "outputId": "bb881795-0999-4dbb-c20c-8fd0a3c3c2de"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (8,6))\n",
    "sns.countplot(x = data['label'], color = 'b')\n",
    "plt.title('Target Classes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_XK2TBjk1Av"
   },
   "source": [
    "As seen above our classes are well balanced. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qe3bf13Gk1Aw"
   },
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We will normalize the input data by dividing the pixel values by 255. \n",
    "\n",
    "We will be implementing Support Vector Machines (SVM) and Convolutional Neural Networks (CNN) to weigh the pros and cons of using such models for image classification. To begin, we will be using the scikit learn implementation of Support Vector Machine (SVM). Using sklearn GridSearch, we will optimize the parameters for these models. We will also build a Convolutional Neural Network (CNN) using PyTorch and TensorFLow. We will create multiple models and manipulate the number of layers and filters and will be comparing the accuracies along the way. \n",
    "\n",
    "SVM constructs a set of hyperplanes in a high dimensional space to separate the classes. Due to the complexity of image classification, we expect deep learning to result in the highest accuracy. We will experiment with the number of Conv2D and Dense layers and use Dropout for regualarization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5UltVbmk1A0"
   },
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "For the evaluation metrics, we plan to use the multi-classes error measures. Instead of having positive/negative as our label values, we plan to have each sign alphabet (all 24 of them), then use suitable variables to calculate specificity and recall for each alphabet. Using this method, we will be able to achieve various insights such as how similar alphabet sign letters are affecting its accuracy/precision of the model, and how different model complexities affect our accuracy/precision of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMMyNOTQk1A3"
   },
   "source": [
    "For CNN, we will be using accuracy as the evaluation metric. As shown above, we have a balanced classes so accuracy was a suitable metric. Had we more time, we would have also liked to consider F1 score to gauge both precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjfimXuBk1A4"
   },
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Selecting Algorithms\n",
    "\n",
    "We began by listing all suitable algorithms that were suitable for our classification problem. We narrowed this down to a list of four: K-nearest Neighbors (KNN), Support Vector Machine (SVM), Decision Tree and Convolutional Neural Network (CNN). From this list, we decided to focus on only SVM and CNN. KNN may not be complex enough for our image classification problem and Decision Trees are prone to overfitting. SVM was deemed suitable as we would like to set decision boundaries that maximize the margins between the classes. CNNs are capable of learning complex patterns and work well for the complicated task of image classification\n",
    "\n",
    "### CNN Baseline Model\n",
    "\n",
    "We began working with CNN by creating a baseline model of Conv2D, MaxPooling and Dense layers. At this point, we were not paying too much attention to the makeup or number of layers. We simply wanted to gauge the accuracy of a simple neural network before proceeding to fine tune.\n",
    "\n",
    "### CNN Overfitting\n",
    "\n",
    "Our baseline model appeared to be slightly overfitting as it achieved 100% training accuracy. However, the testing accuracy was still fairly good at around 92-93%. In attempt of improving the testing accuracy, we researched methods to reduce overfitting and bring the training and testing accuracy closer together. Firstly, we attempted l2 regularization on the weights at various layers. Depending on the learning rate parameter used, the results varied. However, the addition of l2 regularization made training __much__ slower. Instead of the 10 epochs before, training now involved 100+ epochs, and the accuracy learning curves were still increasing when training was complete. We decided to attempt a more efficient strategy to treat the overfitting. Weight initialization was another idea. This resulted in overfitting once again. Finally, we attempted the addition of Dropout layers. This resulted in a reduction of overfitting and maximal testing accuracy of all our solutions. Dropout layers render a proportion of random neurons dead. We were able to increase testing accuracy to 94-95% while reducing training accuracy to 97-98%.\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2__U30ELk1A7"
   },
   "source": [
    "## Results - SVM \n",
    "\n",
    "Our results and interpretation of results can be found in another notebook called \"SVM Model.ipynb\". To summarize, the main finding of the SVM model was that our dataset is too normalized in a sense that we can reduce the dimensionality of the dataset to less than 10% and it still outperforms the validation test. Very small regularization results a overfit in the dataset. However, since data was too normalized, when it sees some ambiguous data, it tend to make mistakes. We also performed data augmentation by performing PCA and HOG to futher analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKyHk0r9k1A_"
   },
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "As we can see form the notebook called \"SVM Model.ipynb\", From the GridSearchCV, we got a validation score of 1.0 for linear and poly kernel, on the other hand rbf kernel shows average score of 0.35. This tells us that our data is too-normalized so that the train dataset's same label dataset are equal to each other. We validated this claim by testing using low-neighbor KNN, and got a result really close to 1. We also tested several different types of gamma value to find optimal gamma value, and came to conclusion to set gamma kernel of 'Scale'. After we augmented the data and tested our hyporthesises using different train and test data, we came to conlcusion thant data was too normalized, which very small regularization results a overfit in the dataset.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Since our testing set had more variabilty than the training set, we need to increase the range of our training dataset. This can be accomplished through data augmentstion methods to introduce more variability into our training dataset. Given more time, we could increase the scope of our training set and introduce more variability for the models to train on. And as we said above, our data was too normalized, thus limited our results. So more data with wide variability would make our results more accurate. \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "For our ethical review, we followed the guidelines from \"Data Science Ethics Checklist\" by deon (\"https://deon.drivendata.org/\"), which contains crucial information to consider such as how data is collected, how data is being stored, how to minimize the bias when making analysis. Our dataset is from Kaggle, which is open-source dataset webpage where all users can view/use for their commercial use. Though the author of this dataset does not mention who was recorded or how many people were being involved in this dataset, we can mitigate most of this privacy issue since the dataset only contains pictures of hands, with pixels of 28 x 28 per image. Our dataset link is in our notebook, to have transparency in our project.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The best performing CNN model attained a training accuracy of 1.0 and a testing accuracy of 0.93. Our worst perfoming model according to training accuracy acquired a training accuracy of 0.96 and a testing accuracy of 0.94. The low varaibilty in accuracies among the training and testing set suggests that our datasets are similar in variability and that the training set covers a good proportion of the testing set. Furthermore, such high accuracies suggest that the model is overfitting on the training set. And lower testing accuracies with higer testing loss suggestes that our testing set had more variability than the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0-aPjhzDk1BB"
   },
   "source": [
    "# Footnotes\n",
    "<a name=\"deafStats\"></a>1.(#deafStats): Mitchell R. E. (2006). How many deaf people are there in the United States? Estimates from the Survey of Income and Program Participation. Journal of deaf studies and deaf education, 11(1), 112–119. https://pubmed.ncbi.nlm.nih.gov/16177267/<br>\n",
    "<a name=\"ASLpaper\"></a>2.(#ASL): Garcia, B., & Viesca, S. A. (2016). Real-time American sign language recognition with convolutional neural networks. Convolutional Neural Networks for Visual Recognition, 2, 225-232. http://cs231n.stanford.edu/reports/2016/pdfs/214_Report.pdf<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btmxv9Jxk1BD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "FinalProject_group031.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "70b8d9a4f79ce26a5c42f30717429203496a780e9421e71e46698b8c787fce83"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
